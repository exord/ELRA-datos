{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/exord/ELRA-datos/blob/main/ELRA_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atycxuXjYMKU"
   },
   "source": [
    "# Celdas preparatorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcBJtiOXOglT"
   },
   "source": [
    "`colab.research.google.com`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0v5m__QsYMKW",
    "outputId": "afcc7ba8-f1a9-4cdc-ad56-35f587b38ddf"
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ELCA_class1\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqWe9ufVO5jA"
   },
   "source": [
    "## Import functions from utils module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_T_9EsblO_6X",
    "outputId": "818b5600-f240-428e-e7fb-95f7f3bec43e"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "    DOWNLOAD_URL = \"https://raw.githubusercontent.com/exord/ELRA-datos/main/utils.py\"\n",
    "    !wget {DOWNLOAD_URL}\n",
    "\n",
    "from utils import anova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tB3D-uS1YMKX"
   },
   "source": [
    "# Obtención de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTSXl66XYMKX"
   },
   "source": [
    "Para estudiar el tema de la evaluación de modelos, vamos a obtener los datos de la última vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2klN5T0lYMKX"
   },
   "outputs": [],
   "source": [
    "AIRLINE_PATH = \"datasets/airline_fatalities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1i50XAvDYMKY",
    "outputId": "ae8892fc-071d-400f-e2fd-eab4a8f47f00"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "        \n",
    "    import tarfile\n",
    "\n",
    "    DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/IAI-UNSAM/datasets/master/\"\n",
    "    AIRLINE_URL = DOWNLOAD_ROOT + \"airline_fatalities/Data-Table\\ 1.csv\"\n",
    "\n",
    "    def fetch_fatalities(AIRLINE_URL=AIRLINE_URL, airline_path=AIRLINE_PATH):\n",
    "        os.makedirs(airline_path, exist_ok=True)\n",
    "        !wget {AIRLINE_URL} -P {airline_path}\n",
    "\n",
    "    # Corramos la función\n",
    "    fetch_fatalities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUI2OSXwYMKY"
   },
   "outputs": [],
   "source": [
    "year, acc, deaths, rate = np.loadtxt(os.path.join(AIRLINE_PATH, 'Data-Table 1.csv'), delimiter=',', skiprows=2, unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "yWmAu867YMKZ",
    "outputId": "5c97a8a7-a4b2-4423-cbbb-644c81343f8c"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(9, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "l0, = ax.plot(year, rate, 'o-', mfc='None', ms=10, mew=1, color='g', label='Muertes / 100 mill. kms')\n",
    "\n",
    "ax.set_xlabel('Año', fontsize=16)\n",
    "ax.set_ylabel('Tasa', fontsize=16)\n",
    "\n",
    "ax.legend(loc=0, fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2Z-yaypYMKZ",
    "outputId": "4a0da187-83ab-494e-a440-00099a2fbf31"
   },
   "outputs": [],
   "source": [
    "rate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxas9fwrYMKa"
   },
   "source": [
    "Vamos a ajustar un modelo lineal simple de la forma:\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x}, \\boldsymbol{\\omega}) = \\omega_0 + \\omega_1 \\mathbf{x}\\;\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MWhEg7FYMKa",
    "outputId": "b1d0c35a-cf2e-48cb-a343-822dd92f424f"
   },
   "outputs": [],
   "source": [
    "X = year.reshape(-1, 1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c7S1gZmOglf",
    "outputId": "7b56b085-7d0a-4b25-e2a7-0ce57f6f4a58"
   },
   "outputs": [],
   "source": [
    "# Le damos forma a los datos para prepararlos para statsmodels\n",
    "X = year.reshape(-1, 1) - year.mean()\n",
    "t = rate.reshape(-1, 1)\n",
    "\n",
    "# Add intercept term\n",
    "X = sm.tools.add_constant(X)\n",
    "print(X[:3])\n",
    "\n",
    "# Ajusta el modelo\n",
    "results = sm.OLS(t.flatten(), X).fit()\n",
    "\n",
    "# Print results\n",
    "print(results.summary())\n",
    "\n",
    "# Compute the predictions\n",
    "y = results.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "Kfr5GyaXYMKb",
    "outputId": "33fce49c-3f61-44dc-c63b-b83a73f30d7e"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(9, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "l0, = ax.plot(year, rate, 'o-', mfc='None', ms=10, mew=1, color='g', label='Muertes / 100 mill. kms')\n",
    "l1, = ax.plot(year, y, 'o-r', lw=3, alpha=0.8)\n",
    "ax.set_xlabel('Año', fontsize=16)\n",
    "ax.set_ylabel('Tasa', fontsize=16)\n",
    "\n",
    "ax.legend(loc=0, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kO1vzyQzYMKb"
   },
   "source": [
    "También calculamos los residuos $\\left\\{r_i\\right\\}$, con $i = 1, \\ldots, N$, definidos como la diferencia entre los valores medidos y la predicción del modelo:\n",
    "\n",
    "$$\n",
    "r_i = t_i - y(\\mathbf{x}_i, \\boldsymbol{\\omega})\\;\\;.\n",
    "$$\n",
    "\n",
    "Una de las cosas para las que usamos los residuos fue para estimar la varianza de los términos de error $\\epsilon_i$ [**Nota**: recordemos que suponemos $\\epsilon_i \\sim N(0, \\sigma^2)$]:\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{var}(\\epsilon)} = \\widehat{\\sigma}^2 = \\frac{1}{N-2}\\sum_{i=1}^N r_i^2\\;\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0e0BIaMYMKb"
   },
   "source": [
    "Ese valor nos sirvió también para hacer inferencia sobre el parámetro $\\omega_1$, y decidir si es significativamente diferente de cero, y para estimar el error que tendrán las predicciones que hagamos con el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HufdQaabYMKc"
   },
   "source": [
    "Pero también podemos usar los residuos para evaluar la performance del modelo de muchas maneras diferentes. Esto se pude hacer para el modelo lineal simple, y es lo que vamos a ver ahora, pero es particularmente útil para el caso del modelo lineal múltiple, donde va a ser más difícil plotear directamente $y(\\mathbf{x}, \\mathbf{w})$. \n",
    "\n",
    "Pero primero tenemos que ver el concepto de *leverage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAXA6yitOgli",
    "outputId": "a4e013a3-b0d8-4565-d000-0481035a5ed8"
   },
   "outputs": [],
   "source": [
    "screg = np.sum((y - t.mean())**2)\n",
    "scres = np.sum((t - y)**2)/(len(t)-2)\n",
    "\n",
    "fratio = screg/scres\n",
    "print('F-statistic = {:.2f} (es decir, ENORME)'.format(fratio))\n",
    "\n",
    "import scipy.stats as st\n",
    "my_f = st.f(dfn=1, dfd=len(t)-2)\n",
    "print('La probabilidad de tener un valor de F-statistic al menos así de alto es {:.2e}'.format(1 - my_f.cdf(fratio)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_kUgsgJOgli",
    "outputId": "9d634f97-381a-442e-8925-32a99479dd91"
   },
   "outputs": [],
   "source": [
    "anova(t.flatten(), t.flatten().mean(), [y,], 1, [3,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vv5zfcbZOglj",
    "outputId": "5d2ec07d-53ff-4393-f8d8-276f796d5827"
   },
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H72Bxx1yYMKc"
   },
   "source": [
    "# Evaluación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8wDroRXYMKc"
   },
   "source": [
    "## Leverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2NobSQSYMKd"
   },
   "source": [
    "El concepto de *leverage* o palanca es da gran utilidad para estudiar el funcionamiento del modelo y la influencia de los datos.\n",
    "\n",
    "Se puede mostrar que el valor predicho de la variable $t$ para un valor del *feature* $x_i$ puede expresarse como una combinación lineal de *todos* los labels.\n",
    "\n",
    "$$\n",
    "y_i = \\hat{\\omega_0} + \\hat{\\omega_1} x_i = \\sum_{k=1}^N h_{ik} t_k\\;\\;,\n",
    "$$\n",
    "donde $h_{ik}$ se define:\n",
    "\n",
    "$$\n",
    "h_{ik}: = \\frac{1}{N} + \\frac{\\left(x_i - \\bar{x}\\right)\\left(x_k - \\bar{x}\\right)}{S_{xx}}\\;\\;,\n",
    "$$\n",
    "\n",
    "con, recordemos, \n",
    "$$\n",
    "\\bar{x} = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
    "$$\n",
    "y\n",
    "$$\n",
    "S_{xx} = \\sum_{i=1}^N \\left(x_i - \\bar{x}\\right)^2\\;\\;.\n",
    "$$\n",
    "\n",
    "Es decir, la predicción es una combinación lineal de los valores observados, pesada por el elemento $h_{ik}$ (de la matriz sombrero / hat). Ese valor $h_{ik}$ nos dice cuánto pesa cada medición $k$ en la predicción del valor $y_i$.\n",
    "\n",
    "Let's compute this for our data, knowing that $\\bar{x}=0$, because of the preprocessing (alright, I'll write the whole thing so you can use it in other cases!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ndPvN3EYMKd"
   },
   "outputs": [],
   "source": [
    "from utils import hat_matrix\n",
    "    \n",
    "# Define HAT matrix, whose diagonal are the leverage values\n",
    "h = hat_matrix(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waNt-4MDYMKe"
   },
   "source": [
    "Esta matriz nos dice cómo influye cada observación en la predicción de cualquier otra observación.\n",
    "\n",
    "Podemos graficar la influencia que cada observación tiene sobre otras (tengan en cuenta que todo esto es independiente de $t$; solo tiene en cuenta los valores de la variable predictora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "stoc74PXYMKe",
    "outputId": "34ac667e-4077-4a60-daef-904145e2a032"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "# Elijo que filas de la matriz voy a plotear.\n",
    "indices_to_plot = [0, 11, -1]\n",
    "\n",
    "for c,i in enumerate(indices_to_plot):\n",
    "    plt.plot(X[:, 1], h[i], 'o', ms=10, mfc='None', label=i, color='C{}'.format(c))\n",
    "    plt.plot(X[i, 1], h[i, i], 'o', ms=10, color='C{}'.format(c))\n",
    "plt.xlabel('Año', fontsize=16)\n",
    "plt.ylabel('Leverage', fontsize=16)\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwP-Ldw_YMKe"
   },
   "source": [
    "En particular, la diagonal de la matriz sombrero, que contiene el nivel de influencia de cada observación sobre su propia predicción, se conoce como *leverage* o palanca de esa observación.\n",
    "\n",
    "Podemos graficarla en función de la variable predictora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "_-IQLHneYMKf",
    "outputId": "4f78b95a-1fee-44e2-a430-7b0972a455c8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "# Elijo que filas de la matriz voy a plotear.\n",
    "plt.plot(X[:, 1], np.diag(h), 'o', ms=10, mfc='None', label=i, color='C{}'.format(c))\n",
    "plt.xlabel('Año', fontsize=16)\n",
    "plt.ylabel('Leverage', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0QslsghYMKf"
   },
   "source": [
    "Como es razonable esperar a partir de la ecuación que define la matriz sombrero (y a partir de la intuición), las observaciones que se encuentran cerca se afectan más fuertemente. Es razonable pensar que se toma en cuenta la influencia local con mayor intensidad que la influencia general o global.\n",
    "\n",
    "Esto será también el puntapié para entender métodos de *kernel*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrKMLqYYYMKf"
   },
   "source": [
    "Antes de pasar al tema de residuos, mencionemos una propiedad más de los *leverage*:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N h_{ii} = 2\\;\\;.\n",
    "$$\n",
    "\n",
    "Más en general, la suma de los *leverage* debe ser igual a la dimensión del vector de parámetros, $\\boldsymbol{\\omega}$, como puede verse de la definición de la matrix sombrero para el caso multidimensional (implementada en el modulo `utils`):\n",
    "\n",
    "$$\n",
    "H = \\boldsymbol{\\Phi}\\;(\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi})^{-1} \\boldsymbol{\\Phi}^T\\;\\;,\n",
    "$$\n",
    "\n",
    "donde $\\boldsymbol{\\Phi}$ es la matrix de diseño que vimos la última clase, y tiene dimensiones $(N x D)$, donde $D$ es el número de funciones de base del modelo lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjrRZnn4YMKg"
   },
   "source": [
    "## Residuos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ39-z0lYMKg"
   },
   "source": [
    "### Residual plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5yzE2r7YMKg"
   },
   "source": [
    "Ahora sí, volvamos a los residuos. \n",
    "\n",
    "Lo primero que uno puede hacer es graficarlos en función de la variable descriptiva, $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "stEMDdE_YMKg",
    "outputId": "865eb252-4a1b-4224-f589-5f83f0baeb07"
   },
   "outputs": [],
   "source": [
    "res = t.flatten() - y\n",
    "\n",
    "# Plot residuals against feature $x_1$.\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(X[:, 1], res, 'ok', mfc='None', ms=10, color='g')\n",
    "\n",
    "ax.axhline(0, color='r', ls=':')\n",
    "# ax.set_xlabel('Predicción / y')\n",
    "ax.set_xlabel('Año')\n",
    "ax.set_ylabel('Residuos')\n",
    "\n",
    "# Add mean on each axis\n",
    "xmean = X[:, 1].mean()\n",
    "ax.axvline(xmean, color='r', ls=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvh3ummRYMKh"
   },
   "source": [
    "Las ecuaciones normales nos garantizaban dos propiedades de los residuos:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N r_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rho(\\mathbf{x}, \\mathbf{r}) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xGG86UgYMKi",
    "outputId": "1f074639-ef46-4b4c-bb70-bd0b0e802a48"
   },
   "outputs": [],
   "source": [
    "print('Suma de los residuos {:.16f}'.format(res.sum()))\n",
    "print('$rho(X, res)$ = {:.16f}'.format(np.corrcoef(X[:, 1].T, res.T)[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezmpGLPoYMKi"
   },
   "source": [
    "De lo que dijimos arriba, se puede desprender, de manera informal, que $\\mathbb{E}(r_i) = 0$, para todo $i = 1, \\ldots, N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsliKquNYMKj"
   },
   "source": [
    "### Varianza de los residuos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNTcOfK6YMKj"
   },
   "source": [
    "Además, es interesante ver qué expresión tiene la variaza de los residuos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cE_BVgbDYMKj"
   },
   "source": [
    "$$\\mathrm{var}(r_i) = \\sigma^2 (1 - h_{ii})\\;\\;\\text{, donde}$$\n",
    "\n",
    "$$h_{ii} = \\frac{1}{N} + \\frac{X_i - \\bar{X}}{S_{xx}}\\text{ es el leverage}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeXRAacJYMKj"
   },
   "source": [
    "De la definición de *leverage* podemos ver que los valores están acotados, de manera que nunca valen más que uno (de manera que la varianza nunca puede ser negativa).\n",
    "\n",
    "Lo que resulta interesante es que una observación que tenga mucho leverage tendrá un residuo cuya varianza es muy inferior al valor de la varianza de su término de error. Esta observación tira del modelo para acercarlo a ella, por lo que genera esa reducción de la varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yj3dVoQcYMKk"
   },
   "source": [
    "Para ver esto, nada como datos sintéticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b21JvyYuYMKk"
   },
   "outputs": [],
   "source": [
    "# defino funciones útiles\n",
    "def make_default_dataset(real_process, sigma=0.5, high_leverage=None, random_seed=20210331):\n",
    "    # Fijo seed\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Defino vector de x\n",
    "    x = np.random.rand(20)\n",
    "\n",
    "    # Por si quiero otra nube de puntos\n",
    "#     x2 = np.random.rand(4) + 2.5\n",
    "#     x = np.concatenate([x, x2])\n",
    "        \n",
    "    x = np.sort(x)\n",
    "\n",
    "    # Agrego un punto con mucha palanca\n",
    "    if high_leverage is not None:\n",
    "        high_leverage_x = np.array(high_leverage)\n",
    "        x = np.append(x, high_leverage_x)\n",
    "    \n",
    "    x_plot = np.linspace(x.min(), x.max(), 100).reshape(-1, 1)\n",
    "\n",
    "    # Error\n",
    "    t = real_process(x) + np.random.randn(len(x)) * sigma\n",
    "    \n",
    "    return x, t, x_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "ChVj_JT5YMKk",
    "outputId": "bf8f7edf-91a3-442c-890d-22cdb79cfb53"
   },
   "outputs": [],
   "source": [
    "def ground_truth(x):\n",
    "    return 3*x + 4 #+ 0.1*x**2\n",
    "\n",
    "# Creo los datos\n",
    "x, t, x_plot = make_default_dataset(ground_truth, high_leverage=3.0)\n",
    "\n",
    "# Plot datos y gt\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(x, t, 'o', ms=10)\n",
    "plt.plot(x_plot, ground_truth(x_plot), '-', color='0.5', label='Proceso real')\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('t', fontsize=16)\n",
    "\n",
    "# Ajusto y ploteo ajuste\n",
    "lr = LinearRegression()\n",
    "lr.fit(x.reshape(-1, 1), t)\n",
    "plt.plot(x_plot, lr.predict(x_plot), '-r', lw=4, alpha=0.5, label='Ajuste')\n",
    "plt.legend(loc=0, fontsize=16)\n",
    "\n",
    "# Calculo los residuos\n",
    "res = t - lr.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQEpf0EfYMKl",
    "outputId": "01456f6e-0e33-40a8-a98e-5d94934ef40a"
   },
   "outputs": [],
   "source": [
    "print(lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16OCa0rxYMKl"
   },
   "source": [
    "Ahora repitamos el proceso de generación de datos (dejando fijas las variables $X$) y calculemos el mejor ajuste y los residuos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZHBRpAKYMKl"
   },
   "outputs": [],
   "source": [
    "nsimu=1000\n",
    "\n",
    "res_ = np.empty((nsimu, len(x)))\n",
    "t_ = np.empty((nsimu, len(x)))\n",
    "sigma = 0.5\n",
    "for i in range(nsimu):\n",
    "    # Nuevo dataset\n",
    "    t_new = ground_truth(x) + np.random.randn(len(x)) * sigma\n",
    "    \n",
    "    t_[i] = t_new\n",
    "    # Ajusta y predice\n",
    "    lr.fit(x.reshape(-1, 1), t_new)\n",
    "    y = lr.predict(x.reshape(-1, 1))\n",
    "    \n",
    "    # Calcula residuos y guarda\n",
    "    res_[i] = t_new - y\n",
    "    \n",
    "# Ordenemos los residuos por valor de x\n",
    "res_ = res_[:, np.argsort(x)]\n",
    "x_ = np.sort(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "vC3OI_oMYMKl",
    "outputId": "f824778c-6f79-4f49-fe3e-a113785a2f00"
   },
   "outputs": [],
   "source": [
    "# Plot datos y gt\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(x, t, 'o', ms=10)\n",
    "plt.plot(x, t_new, 'o', ms=10)\n",
    "plt.plot(x_plot, ground_truth(x_plot), '-', color='0.5', label='Proceso real')\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('t', fontsize=16)\n",
    "\n",
    "# Ajusto y ploteo ajuste\n",
    "lr = LinearRegression()\n",
    "lr.fit(x.reshape(-1, 1), t)\n",
    "plt.plot(x_plot, lr.predict(x_plot), '-r', lw=4, alpha=0.5, label='Ajuste')\n",
    "plt.legend(loc=0, fontsize=16)\n",
    "\n",
    "# Calculo los residuos\n",
    "res = t - lr.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALtZLAX5YMKm"
   },
   "source": [
    "Ahora hagamos histogramas de los residuos para distintos valores de $X$. Recordemos que, como nosotros definimos el proceso real, sabemos con certeza que $\\sigma^2 = (0.5)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMev8SgnYMKm",
    "outputId": "24903172-3ff6-454a-936d-361f1954591a"
   },
   "outputs": [],
   "source": [
    "# Veamos la dispersión de los datos entorno de la media del valor real\n",
    "np.std(t_ - ground_truth(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "G-2upe0IYMKm",
    "outputId": "51b433f3-4a7a-48c1-8abb-f7b1ea467158"
   },
   "outputs": [],
   "source": [
    "indices_to_plot = [0, 11, -1]\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "for i in indices_to_plot:\n",
    "    plt.hist(res_[:, i], 25, label='X = {:.1f} (std(res) = {:.2f})'.format(x_[i], res_[:, i].std()), histtype='step', lw=3)\n",
    "plt.xlabel('Residuals of observation X')\n",
    "plt.legend(loc=0, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX9N_LwlYMKm"
   },
   "source": [
    "### *Leverage* vs. Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3bF8nArYMKn"
   },
   "source": [
    "Un plot muy relevante es el de los valores de *leverage* en función de los residuos.\n",
    "\n",
    "Sigamos por ahora con nuestros datos sintéticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4Xlkxp5YMKn"
   },
   "outputs": [],
   "source": [
    "# Calculamos leverage para los datos sintéticos\n",
    "h = np.diag(hat_matrix(x.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "Vr1Tt1lSYMKn",
    "outputId": "9f7724b5-b87f-4c3c-f5af-c55a07b16657"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(h, res, 'o', ms=10)\n",
    "plt.axhline(0, color='r', ls=':')\n",
    "# plt.gca().set_xscale('log')\n",
    "\n",
    "plt.ylabel('Residuals', fontsize=16)\n",
    "plt.xlabel('Leverage', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c13Zfg2RYMKn"
   },
   "source": [
    "Otra forma de ver los *leverages* es con un boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "L1fxPvJ-YMKn",
    "outputId": "38d6c442-a7f6-4574-fed8-8ca03d541751"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "bp = ax.boxplot(h)\n",
    "ax2 = fig.add_subplot(122)\n",
    "hp = ax2.hist(h, histtype='step', color='k', lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5q8b5SuBYMKo"
   },
   "source": [
    "### Residuos estandarizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT-63AuiYMKo"
   },
   "source": [
    "Una pequeña precisión que puede hacerse es definir los residuos estandarizados, en los que dividimos a cada residuo por la raíz cuadrada de varianza esperada.\n",
    "\n",
    "$$\n",
    "R_i = \\frac{r_i}{\\sqrt{\\widehat{\\mathrm{var}}(r_i)}} = \\frac{r_i}{\\sqrt{\\widehat{\\sigma}^2 (1 - h_{ii})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rt36tJfyYMKo"
   },
   "outputs": [],
   "source": [
    "sigma2hat = np.sum(res**2)/(len(res) - 2)\n",
    "stres = res / np.sqrt(sigma2hat * (1 - h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hflMeuYYMKo"
   },
   "source": [
    "Los plots que mencionamos arriba pueden volver a hacerse con los resiudos estandarizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zckHr3KJYMKo",
    "outputId": "213b0231-9c7b-4b07-97dc-e4da40abfb23"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.semilogx(h, stres, 'o', ms=10)\n",
    "plt.axhline(0, color='r', ls=':')\n",
    "# plt.gca().set_xscale('log')\n",
    "\n",
    "plt.ylabel('Residuos Estandarizados', fontsize=16)\n",
    "plt.xlabel('Leverage', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPRmCCd8YMKp"
   },
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYHTj8M-YMKp"
   },
   "source": [
    "Uno de los tests importantes a hacer es el de puntos atípicos (*outliers*). Si por alguna razón los datos tienen algún valor que no proviene del proceso real, eso puede modificar  el resultado del ajuste y, en particular, sesgarlo fuertemente.\n",
    "\n",
    "Por ejemplo, agarremos los datos sintéticos y hagamos que uno de los puntos se vuelva un outlier, poniendo un valor a mano. Para que el efecto sea claro, tenemos que hacerlo en un punto que tenga palanca. Veamos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNorH7o-Oglx"
   },
   "outputs": [],
   "source": [
    "def ground_truth(x):\n",
    "    return 3*x + 4 #+ 0.1*x**2\n",
    "\n",
    "# Function to make random dataset base on graound truth\n",
    "def make_default_dataset(real_process, sigma=0.5, high_leverage=None, random_seed=20210331):\n",
    "    # Fijo seed\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Defino vector de x\n",
    "    x = np.random.rand(20)\n",
    "\n",
    "    # Por si quiero otra nube de puntos\n",
    "#     x2 = np.random.rand(4) + 2.5\n",
    "#     x = np.concatenate([x, x2])\n",
    "        \n",
    "    x = np.sort(x)\n",
    "\n",
    "    # Agrego un punto con mucha palanca\n",
    "    if high_leverage is not None:\n",
    "        high_leverage_x = np.array(high_leverage)\n",
    "        x = np.append(x, high_leverage_x)\n",
    "    \n",
    "    x_plot = np.linspace(x.min(), x.max(), 100).reshape(-1, 1)\n",
    "\n",
    "    # Error\n",
    "    t = real_process(x) + np.random.randn(len(x)) * sigma\n",
    "    \n",
    "    return x, t, x_plot\n",
    "\n",
    "x, t, x_plot = make_default_dataset(ground_truth, high_leverage=3)\n",
    "\n",
    "# Recompute leverage\n",
    "h = np.diag(hat_matrix(x.reshape(-1, 1)))\n",
    "\n",
    "# Define an outlier observation.\n",
    "t[-1] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAaud6UtYMKp"
   },
   "outputs": [],
   "source": [
    "\n",
    "x, t, x_plot = make_default_dataset(ground_truth, high_leverage=3.0)\n",
    "\n",
    "# Recompute leverage\n",
    "h = np.diag(hat_matrix(x.reshape(-1, 1)))\n",
    "\n",
    "# Define an outlier observation.\n",
    "t[-1] =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlr4_MJkYMKp",
    "outputId": "8a3f12c1-db24-46bb-a1ef-5c4e29b08483"
   },
   "outputs": [],
   "source": [
    "# Plot datos y gt\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(x, t, 'o', ms=10)\n",
    "plt.plot(x_plot, ground_truth(x_plot), '-', color='0.5', label='Proceso real')\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('t', fontsize=16)\n",
    "\n",
    "# Ajusto y ploteo ajuste\n",
    "lr = LinearRegression()\n",
    "lr.fit(x.reshape(-1, 1), t)\n",
    "plt.plot(x_plot, lr.predict(x_plot), '-r', lw=4, alpha=0.5, label='Ajuste')\n",
    "plt.legend(loc=0, fontsize=16)\n",
    "\n",
    "# Calculo los residuos\n",
    "res = t - lr.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMWqsKN3YMKp"
   },
   "source": [
    "Sin embargo, miren qué pasa si miramos el gráfico de leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNc1xbUPYMKq",
    "outputId": "acd88112-f00f-4377-de99-ad35a95ead5f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(h, res, 'o', ms=10)\n",
    "plt.axhline(0, color='r', ls=':')\n",
    "# plt.gca().set_xscale('log')\n",
    "\n",
    "plt.ylabel('Residuos', fontsize=16)\n",
    "plt.xlabel('Leverage', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbD2Q-L-YMKq",
    "outputId": "46dd2416-c345-49af-937d-05b816fad652"
   },
   "outputs": [],
   "source": [
    "plt.plot(x, res, 'or')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyXPLWyKYMKq"
   },
   "source": [
    "Si bien el outlier está bastante afuera, no llama mucho la atención. Es claro lo que pasa: el outlier ya arrastró la solución y entonces el residuo es relativamente pequeño para ese punto.\n",
    "\n",
    "**¿Y entonces? ¿Cómo hacemos? ¿Ideas? ¿Ideas que funcionen en muchas dimensiones?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gf9xD4FXYMKq"
   },
   "source": [
    "### Validación Cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Wi3Pb3vYMKq"
   },
   "source": [
    "Una posibilidad, es hacer uso de una herramienta que será central en la materia para detectar y evitar el sobre-ajuste (*overfitting*), que se llama validación cruzada (*cross-validation*). \n",
    "\n",
    "Acá la vamos a usar para detectar el punto que se escapa. De alguna forma, también se trata de sobreajuste, porque el modelo intenta agarrar todo el set de entrenamiento, en lugar de preocuparse por reproducir la tendencia general, acá dada por el proceso real (¡que por suerte acá conocemos!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4n7c0GT2YMKr"
   },
   "source": [
    "La idea va a ser construir sub-sets de datos, a partir del set original, sacando un punto por vez. Es decir, se construyen $N$ conjuntos de $N-1$ datos, y se ajusta un modelo para cada uno de esos datos nuevos. Después, se compara la predicción que hace cada modelo para el punto que sacamos con la que hace el modelo ajustado a todos los datos.\n",
    "\n",
    "La intuición es que cuando saquemos el outlier que está infuyendo en el ajuste, la predicción cambiará rotundamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywAVljx3YMKr"
   },
   "source": [
    "Esto se llama *Leave-one-out cross-validation*, porque los puntos se van tomando de a uno. Hay muchos más sabores de validación cruzada, algunos de los cuales veremos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_9dn6GTYMKr",
    "outputId": "9f12ef99-b68a-4f74-b37e-a76dd6308e96"
   },
   "outputs": [],
   "source": [
    "# Implementación de LOOCV en sklearn\n",
    "from sklearn.model_selection import LeaveOneOut, LeavePOut, cross_val_predict\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "# loo = LeavePOut(3)\n",
    "for train, test in loo.split(x):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkaSJCskYMKr"
   },
   "outputs": [],
   "source": [
    "# Compute the predictions of Linear Regressor leaving each point out\n",
    "y_iout = cross_val_predict(lr, x.reshape(-1, 1), t, cv=loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qGaKHzJNYMKr",
    "outputId": "4e806450-a2ea-43a9-94e0-09bca93b30b2"
   },
   "outputs": [],
   "source": [
    "print(y_iout.shape, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMFm0I-0YMKs"
   },
   "outputs": [],
   "source": [
    "# pero para entenderlo, está bueno hacerlo a mnos, aunque sea una vez.\n",
    "y_iout_mano = np.empty_like(t)\n",
    "\n",
    "for i, [train, test] in enumerate(loo.split(x)):\n",
    "    x_i = x[train]\n",
    "    t_i = t[train]\n",
    "    \n",
    "    lr.fit(x_i.reshape(-1, 1), t_i)\n",
    "    y_ii = lr.predict(x[test].reshape(-1, 1))\n",
    "    \n",
    "    y_iout_mano[i] = y_ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Z_l625gYMKs",
    "outputId": "785eb313-c0b3-46eb-8261-fc332644eab1"
   },
   "outputs": [],
   "source": [
    "np.allclose(y_iout, y_iout_mano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIaaDl1tYMKs",
    "outputId": "b36fb4a1-37a0-408a-eb79-8cbcbb38d245"
   },
   "outputs": [],
   "source": [
    "lr.fit(x.reshape(-1, 1), t)\n",
    "y = lr.predict(x.reshape(-1, 1))\n",
    "plt.plot(x, (y - y_iout), 'or', ms=10, mfc='None')\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('$Y_i - Y_{i(i)}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WfzNNsqYMKs"
   },
   "source": [
    "Podemos ver que claramente el outlier salta a la vista.\n",
    "\n",
    "Para casos menos obvios, podemos definir un estadístico que se compara con alguna distribución razonable, pero no vamos a entrar en esos detalles ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0eZj7diYMKt"
   },
   "source": [
    "### Relevancia de una observación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPotND-fYMKt"
   },
   "source": [
    "Obviamente, un outlier como el que vimos recién es una observación muy influyente, que determina fuertemente el resultado del ajuste.\n",
    "\n",
    "Pero si ese mismo outlier hubiera tenido menos palanca, la cosa hubiera sido diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCeCbsbcYMKt"
   },
   "source": [
    "#### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skQeEvaDYMKt"
   },
   "source": [
    "* Modifiquen la posición y el valor del outlier. Vean qué influencia tiene en cada caso.\n",
    "* Discutan qué combinación tiene que darse para que una medición sea influyente.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lHHPWLyYMKt"
   },
   "source": [
    "# Comparación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuugyV9oYMKu"
   },
   "source": [
    "## Analysis of Variace (ANOVA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMqEq6L4YMKu"
   },
   "source": [
    "El análisis de varianza (o ANOVA) está basado en el hecho de que la siguiente expresión es válida en general para $y$ obtenido a partir modelos de cuadrados mínimos:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\sum_{i=1}^N\\left(t - \\bar{t}\\right)^2}_{SC_\\mathrm{tot}} = \\underbrace{\\sum_{i=1}^N\\left(t - y\\right)^2}_{SC_\\mathrm{res}} + \\underbrace{\\sum_{i=1}^N\\left(y - \\bar{t}\\right)^2}_{SC_\\mathrm{reg}}\\;\\;,\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$\n",
    "\\bar{t} = \\frac{1}{N}\\sum_{j=1}^N t_j\\;\\;,\n",
    "$$\n",
    "\n",
    "puede pensarse como la predicción del modelo más simple que existe: ¡una constante!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duNviICpYMKu"
   },
   "source": [
    "**Nota**: siempre es muy importante tener un modelo de referencia con el que podamos comparar nuestro modelo. En el caso del modelo lineal simple, a menudo se usa el modelo constante aún más simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52PtKCrpYMKu"
   },
   "source": [
    "Podemos construir un estadístico que, intuitivamente, debería aumentar a medida que aumenta la capacidad del modelo para describir los datos:\n",
    "\n",
    "$$\n",
    "F\\text{-statistic} = \\frac{SC_\\mathrm{reg}}{SC_\\mathrm{res}/(N-2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quLXA1urYMKv"
   },
   "source": [
    "Se puede demostrar que, bajo la hipótesis de que el modelo es incorrecto (es decir, que ambos modelos explican bien los datos de manera equivalente), $ F\\text{-statistic}$ sigue una distribución conocida (la distribución F, con 1, y $N -2$ grados de libertad). Grafiquemos esa distribución para nuestro valor de $N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "3i29mT6bYMKv",
    "outputId": "71169339-107c-4ee0-e9d9-66182737473a"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "my_f = st.f(dfn=1, dfd=len(t)-2)\n",
    "\n",
    "xx = np.linspace(0, 5, 100)\n",
    "plt.plot(xx, my_f.pdf(xx))\n",
    "plt.xlabel('F-statistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxH5hXIrYMKv"
   },
   "source": [
    "Ahora, si el valor de $ F \\text{-statistic}$ es muy grande, entonces se puede rechazar la idea de que ambos modelos explican los datos de manera equivalente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xi9al8-qOgl4"
   },
   "source": [
    "### Ejemplo con Airlines fatalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yX0KvRcyOgl4"
   },
   "source": [
    "**Base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMmYXZfROgl4"
   },
   "outputs": [],
   "source": [
    "year, acc, deaths, rate = np.loadtxt(os.path.join(AIRLINE_PATH, 'Data-Table 1.csv'), delimiter=',', skiprows=2, unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "jffuA4qkOgl5",
    "outputId": "53b4a090-5e3f-48e0-d89e-025a3ebab2d0"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(9, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "l0, = ax.plot(year, rate, 'o-', mfc='None', ms=10, mew=1, color='g', label='Muertes / 100 mill. kms')\n",
    "\n",
    "ax.set_xlabel('Año', fontsize=16)\n",
    "ax.set_ylabel('Tasa', fontsize=16)\n",
    "\n",
    "ax.legend(loc=0, fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZTTu0v4Ogl5",
    "outputId": "a99c731c-69fc-40e1-a42f-c497959f07be"
   },
   "outputs": [],
   "source": [
    "# Le damos forma a los datos para prepararlos para statsmodels\n",
    "X = year.reshape(-1, 1) - year.mean()\n",
    "t = rate.reshape(-1, 1)\n",
    "\n",
    "# Add intercept term\n",
    "X = sm.tools.add_constant(X)\n",
    "\n",
    "# Ajusta el modelo\n",
    "results = sm.OLS(t, X).fit()\n",
    "\n",
    "# Print results\n",
    "print(results.summary())\n",
    "\n",
    "# Compute the predictions\n",
    "y = results.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVt4PPJeYMKv",
    "outputId": "03b643d5-43ec-4bbf-c85c-b76e93b82fea"
   },
   "outputs": [],
   "source": [
    "anova(t.flatten(), t.flatten().mean(), [y,], 1, [2,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nm9EsYBMOgl5"
   },
   "source": [
    "**Degree-two polynomial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nphA2mdhOgl6",
    "outputId": "14a44bd9-1be8-4b0c-d5bf-b489d0bf4ba3"
   },
   "outputs": [],
   "source": [
    "# Le damos forma a los datos para prepararlos para statsmodels\n",
    "X = year.reshape(-1, 1) - year.mean()\n",
    "t = rate.reshape(-1, 1)\n",
    "\n",
    "# Add intercept term and degree two\n",
    "X = np.vander(X.flatten(), 3, increasing=True)\n",
    "print(X[:])\n",
    "      \n",
    "# Ajusta el modelo\n",
    "results = sm.OLS(t, X).fit()\n",
    "\n",
    "# Print results\n",
    "print(results.summary())\n",
    "\n",
    "# Compute the predictions\n",
    "y2 = results.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wERYkWHnOgl6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "VRi6EmiEOgl6",
    "outputId": "38c6ef0e-bef3-44a6-ec01-afaaed7386b0"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(9, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "l0, = ax.plot(year, rate, 'o-', mfc='None', ms=10, mew=1, color='g', label='Muertes / 100 mill. kms')\n",
    "l1, = ax.plot(year, y, 'o-r', lw=3, alpha=0.8)\n",
    "l2, = ax.plot(year, y2, 'o-b', lw=3, alpha=0.8)\n",
    "ax.set_xlabel('Año', fontsize=16)\n",
    "ax.set_ylabel('Tasa', fontsize=16)\n",
    "\n",
    "ax.legend(loc=0, fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VN9pqnTcOgl6",
    "outputId": "63724fbd-8066-45dd-cae4-30e35c69856a"
   },
   "outputs": [],
   "source": [
    "anova(t.flatten(), t.flatten().mean(), [y2,], 1, [3,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sN_9qWmYMKw"
   },
   "source": [
    "**¿Qué piensan que vamos a obtener del ANOVA en este caso?**\n",
    "\n",
    "Hagámoslo. Pero ahora el modelo de base es el modelo lineal simple, así que la distribución con la que hay que comparar es nuevamente una $F_{1, N-3}$ (porque la diferencia entre ambos modelos es un solo grado de libertad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iePHpgSfOgl7",
    "outputId": "2822492c-59bd-49ab-da48-acd1d755a962"
   },
   "outputs": [],
   "source": [
    "anova(t.flatten(), y, [y2,], 2, [3,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZdYhCKqOgl7"
   },
   "source": [
    "**Conclusión**. En este caso, no podemos decir que sea preferible el modelo con un polinomio sobre el modelo lineal simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGxYtcBMYMKx"
   },
   "source": [
    "## Coeficiente de determinación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pe9Uz8BdYMKx"
   },
   "source": [
    "Un concepto relacionado es el coeficiente de determinación\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{SC_\\mathrm{tot} - SC_\\mathrm{res}}{SC_\\mathrm{tot}}\\;\\;,\n",
    "$$\n",
    "\n",
    "que toma valores entre 0 y 1, y de alguna manera refleja qué parte de la varianza de los datos es explicada por el modelo. Existe una implementación de esto en `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1POTum8YMKx",
    "outputId": "4343ef2c-9a40-40cd-ed81-16667c88fb9f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print('R^2 (lineal) = {:.3f}'.format(r2_score(t, y)))\n",
    "print('R^2 (quadratic) = {:.3f}'.format(r2_score(t, y2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjSJOuDYYMKy"
   },
   "source": [
    "Esto significa que alrededor del 73% de la varianza desaparece con el modelo lineal simple y un 76% con el modelo lineal múltiple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlA7nDUiYMKy"
   },
   "source": [
    "# Su turno!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "She7h7q-YMKy"
   },
   "source": [
    "Les proponemos que exploren el dataset de alumnos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2g-e_x-7YMKy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/IAI-UNSAM/datasets/master/students/student-mat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "noiGOFxaYMKz",
    "outputId": "2f41fa51-6f1c-422e-c2c9-e1b5ea5bf4b2"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AedVJl54YMKz"
   },
   "source": [
    "Una descripción de cada columna puede encontrarse en la [página de Kaggle](https://www.kaggle.com/uciml/student-alcohol-consumption) de la que sacamos estos datos. \n",
    "\n",
    "* Les proponemos jugar a predecir el valor del examen final ($G3$) en base a cualquiera de lo o los features que tienen a disposición.\n",
    "* Elijan criteriosamente un primer feature y después vayan agregando variables a medida que los datos lo requiran. Para esto último, usen el ANOVA.\n",
    "* Evaluen el modelo mirando los residuos y haciendo plots equivalentes a los que usamos durante este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvEKOAcZYMK0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "04_LinearModels_Evaluation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
